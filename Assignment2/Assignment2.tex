\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{flafter}
\usepackage{subfig}
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage[algo2e]{algorithm2e}
%\graphicspath{ {/Users/tarinichandra/Desktop/Pothole-Detection} }

\begin{document}

\begin{titlepage}
	\centering
	%\includegraphics[width=0.15\textwidth]{IIIT-B_logo.jpg}\par\vspace{1cm}
	{\scshape\LARGE International Institute of Information Technology Bangalore \par}
	\vspace{1cm}
	{\scshape\Large Endsem Assignment\par}
	{\Large Foundation of Big Data Algorithms\par}
	\vspace{1.5cm}
	{\Large\itshape Akanksha Dwivedi - MT2016006\par}
	{\Large\itshape Kuldeep Singh - MT2016073\par}
	{\Large\itshape Tarini Chandrashekhar - MT2016144\par}
	\vfill
	
	Faculty : \par
	Prof. G Srinivasa Raghavan

	\vfill

% Bottom of the page
	{\large \today\par}
\end{titlepage}


\tableofcontents
%\listoffigures
%\listoftables
\newpage
\section{Question 3}
\subsection{Question 3c.}

"D" = Dictionary of strings
As we already proved in question 3a),

\begin{equation*}
    Upper bound on \textit{number of ways to pick k-samples out of length n stream} = 2^{nh}
\end{equation*}

As we later proved by Chernoff's bound in 3b), upper bound on \textit{difference between expected number of i-symbols and the actual number of i-symbols in a length k sample } is given by:
\begin{equation*}
    p(|n_i-np_i| > \epsilon p_i) \leq \frac{\theta\epsilon^2}{2}
\end{equation*}
where $\theta = \Sigma = p_i$

$\Sigma p_i$ can be written as $np_i$ so, that makes the upper bound $\frac{np_i\epsilon^2}{2}$. We want the $\epsilon$ to be as small as possible so, we differentiate it w.r.t $\epsilon$, which gives us:

\begin{equation*}
    c = \epsilon np_i
\end{equation*}
This bound is for one sample, so for k samples, the bound becomes c x c x ...k times i.e $c^k$.

Total length of D is the product of both upper bounds, i.e
\begin{equation*}

length(D) = c^k.2^{nh} = O(c^k.2^{nh})    
\end{equation*}

\subsection{Question 3d.}
S = \Bigg {0S  if S \notin D \\
1 (#S) if S \in D}

To prove: Expected length of S' is upper bounded by $(1+\delta)nh$, where n is the length of S.

Let's define h, i.e. entropy.

\begin{equation*}
    

h = -\sum_{i = 1}^{k} p_ilogp_i
= -(p_1log_1 + p_2logp_2 + \dots + p_klogp_k)
nh = -(np_1logp_1 + np_2logp_2 + \dots np_klogp_k)
nh = -log(p_1^{np_1} \dots p_k^{np_k})
nh = log(p_1^{-np_1}\dotsp_k^{-np_k})

(1+\delta)nh = log(p_1^{-np_1}\dotsp_k^{-np_k})^{1+\delta}

Let the expected length of S' be X.We need to to prove the upper bound on:
\end{equation*}


\begin{equation*}
Pr( X \geq (1+\delta)nh )     
\end{equation*}

where $a = (1+\delta)nh $.

By Chernoff bounds,the following lemma holds good for $0\leq\delta\leq$,

\begin{equation*}
    Pr[ X \geq (1+\delta)\mu ] \leq e^{\frac{-\mu\delta^2}{3}}   
\end{equation*}

Here, 
\begin{equation*}

    Pr( X \geq (1+\delta)nh ) \leq \frac{M_X(t)}{e^{t(1+\delta)nh}}
    \leq \frac{E[\Pi e^{t\Sigma p_i }}{e^{t(1+\delta)nh}}
    \leq e^{n((1+\delta)h -\mu - (1+\delta)hlog(\frac{1+\delta}{\mu}))}

\end{equation*}

This proves:
\begin{equation*}
     Pr[ X \geq (1+\delta)nh ] \leq e^{\frac{-\mu\delta^2}{3}}   
\end{equation*}

Therefore, the upper bound on the above inequality is proved.


\section*{Question 4}
Let $u = {x_1,x_2}$ , i.e. u is 2-D Gaussian vector, so it shows a bivariate normal distribution $\Nu(x,\mu,\Sigma)$ where, 

\begin{equation*}
\mu = \begin{bmatrix}
\mu_1\\ 
\mu_2
\end{bmatrix}
\end{equation*}

\begin{equation*}
\Sigma = \begin{bmatrix}
\Sigma_{11} & \Sigma_{12} \\ 
\Sigma_{21} & \Sigma_{22} 
\end{bmatrix}
\end{equation*}

Note that since $\Sigma$ is a symmetric matrix, $\Sigma = \Sigma^T$.
The joint density function of u is :

\begin{equation*}
u(x) = u(x_1,x_2) = \frac{1}{(2\pi )^{n/2}|\Sigma|^{1/2}}exp[-\frac{1}{2}(x-\mu))^T\Sigma^{-1}(x-\mu)] =  \frac{1}{(2\pi )^{n/2}|\Sigma|^{1/2}}exp[-\frac{1}{2}Q(x_1,x_2)]
\end{equation*}

where Q is defined as:

\begin{equation*}

Q(x_1,x_2) = (x-\mu)^T\Sigma^{-1}(x-\mu) 
		 = [(x_1-\mu_1)^T,(x_2-\mu_2)^T]\begin{bmatrix}\Sigma^{11} & \Sigma^{12} \\ 
\Sigma^{21} & \Sigma^{22} 
\end{bmatrix} \begin{bmatrix}x_1-\mu_1 \\ x_2-\mu_2 \end{bmatrix}

\end{equation*}

We assume that,
\begin{equation*}
\Sigma^{-1} = \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\ 
\Sigma_{21} & \Sigma_{22} 
\end{bmatrix}^{-1}  = \begin{bmatrix}\Sigma^{11} & \Sigma^{12} \\ 
\Sigma^{21} & \Sigma^{22} 
\end{bmatrix}_1
\end{equation*}

So, in order to find the conditional distribution of $x_1$ with respect to $x_2$ and vice-versa, we use the conditional probability theorem, i.e

\begin{equation*}
f(x_1|x_2) = \frac{f(x_1,x_2)}{f(x_2} , and
\end{equation*}

\begin{equation*}
f(x_2|x_1) = \frac{f(x_1,x_2)}{f(x_1)}
\end{equation*} 

where we already defined the joint density function $f(x_1,x_2)$ above and now, we define the individual distributions for $x_1$ and $x_2$, assuming that both components of the 2-D Gaussian vector, u, are already normal with aforementioned defined fixed means and covariance matrices with respect to one another. 

\begin{equation*}
f(x_1) = \frac{1}{(2\pi )^{c/2}|\Sigma|^{1/2}}exp[-\frac{1}{2}(x_1-\mu_1))^T\Sigma_{11}^{-1}(x_1-\mu_1)] 
\end{equation*}
\begin{equation*}
f(x_2) = \frac{1}{(2\pi )^{d/2}|\Sigma|^{1/2}}exp[-\frac{1}{2}(x_2-\mu_2))^T\Sigma_{22}^{-1}(x_2-\mu_2)]
\end{equation*}
assuming c and d are dimensions of $x_1$ and $x_2$ components of u.

So,
\begin{equation*}
f(x_1|x_2) = \frac{ \frac{1}{(2\pi )^{n/2}|\Sigma|^{1/2}}exp[-\frac{1}{2}Q(x_1,x_2)]}{\frac{1}{(2\pi )^{d/2}|\Sigma|^{1/2}}exp[-\frac{1}{2}(x_2-\mu_2))^T\Sigma_{22}^{-1}(x_2-\mu_2)]}
\end{equation*}

\begin{equation*}
f(x_2|x_1) = \frac{\frac{1}{(2\pi )^{n/2}|\Sigma|^{1/2}}exp[-\frac{1}{2}Q(x_1,x_2)]}{\frac{1}{(2\pi )^{c/2}|\Sigma|^{1/2}}exp[-\frac{1}{2}(x_1-\mu_1))^T\Sigma_{11}^{-1}(x_1-\mu_1)] 
}
\end{equation*}

Instead of drawing from a proposed distribution from each dimension, then accepting/rejecting it, in Gibbs sampler, we get a value for that dimension according the variable's conditional distribution, where we calculate a value for a particular dimension, keeping all other values fixed.Let t be the number of states. Following is the algorithm:

\begin{algorithm}[H]
\SetAlgoLined
 1. Set t = 0.\\
 2. Generate an initial state $x^{(0)}$.\\
 3. Repeat k-times. \\
\tab set t = t + 1\\
\tab for each dimension i= 1..D  (\textit{Here, D=2})\\
\tab draw $x_i$ from $p(x_i|x_1,x_2,...,x_{i-1},x_{i+1},...,x_D)$

\end{algorithm}

In order to sample from this distribution using a Gibbs sampler, we need to have in hand the conditional distributions for variables/dimensions $x_1$ and $x_2$:

So, we calculate $p(x_1|x_2^{(t-1)})$ i.e the conditional for component $x_1$ for state t, keeping the component $x_2$ for the previous state t-1 constant,since the value of $x_2$ for state t isn't calculated yet. After this, we calculate $p(x_2|x_1^{(t)})$, keeping the recently calculated $x_1^{(t})$ fixed. 

This is how we use Gibbs sampling to sample the 2-D Gaussian vector u.
\section{Question 5}
Devise a rejection sampling procedure to sample from a beta distribution (a common distribution in ML during Bayesian analysis). The density of the Beta distribution, defined on [0, 1] is given as:
\begin{equation*}
    p(x) = \frac{1}{\beta(a,b)}x^{a-1}(1-x)^{b-1}
\end{equation*}
where $\beta$(a, b) is the beta function defined as $\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$. $\Gamma$(x) is the Gamma function - the generalization of the factorial for real numbers. The gamma function has the nice property that $\Gamma$(x + 1) = x$\Gamma$(x). Try to be creative in the choice of the reference function for the rejection sampling.

\subsection{Solution: }
To develop a rejection sampling procedure from a beta distribution whose density function is
\begin{equation}
    p(x) = \frac{1}{\beta(a,b)}x^{a-1}(1-x)^{b-1}
\end{equation}
where 
\begin{equation*}
    \beta(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}
\end{equation*}
The equation 1 can be written as:
\begin{equation}
    p(x) = \frac{f(x)}{constant}
\end{equation}
and
\begin{equation}
    f(x) = x^{a-1}(1-x)^{b-1}
\end{equation}
Now, the rejection sampling procedure for beta distribution can be described in two steps:
\begin{itemize}
    \item If \textbf{a $\geq$ 1}, \textbf{b $\geq$ 1}:
    \newline
    In this case, the beta density function is bounded i.e. we can bound the beta distribution with a uniform envelope with height as maximum value of f(x). To find the maximum value of f(x), differentiate f(x) wrt x and equating to 0.
    \begin{equation*}
        \frac{d}{dx}f(x) = x^{a-1}(1-x)^{b-1} = 0
    \end{equation*}
    \begin{equation*}
        (a-1)x^{a-2}(1-x)^{b-1} = x^{a-1}(b-1)(1-x)^{b-2}
    \end{equation*}
    \begin{equation*}
        x = \frac{a-1}{a+b-2}
    \end{equation*}
    So, the maximum value of f(x) is Max, as follows:
    \begin{equation*}
        Max = \frac{(a-1)^{a-1}(b-1)^{b-1}}{(a+b-2)^{a+b-2}}
    \end{equation*}
    \newline
    and the bounding uniform envelope around beta distribution is a bounding rectangle with height as the above Max value. We generate points uniformly inside this rectangle and those above the beta distribution region are rejected. Thus, rejection sampling procedure is as follows:
    \begin{algorithm}[H]
    Step-1: Obtain a sample \textbf{y} from \textbf{Y} distribution where Y $\sim$ U[0,1] and U denotes the uniform distribution with unit interval.\;
    \\Step-2: Generate a sample \textbf{u} from the distribution U[0, Max].\; 
    \\Step-3:  Check whether u $<$ f(y) or not.
    \\\hfill Step-3a: If this condition holds, Accept \textbf{y} as a sample drawn from f function.\;
    \\\hfill Step-3b: Else, Reject \textbf{y}, return to the step-1 and repeat the procedure.\;
    \caption{Rejection Sampling Procedure-I}
    \end{algorithm}
    
    \item
    If \textbf{a} $\in$ (0,1) and \textbf{b $\geq$ 1} or \textbf{b} $\in$ (0,1) and \textbf{a $\geq$ 1}, then we cannot get a flat bounding envelope, we will take Y = U$^{\frac{1}{a}}$ [or U$^{\frac{1}{b}}$] where U $\sim$ U[0,1] and g(x) = ax$^{a-1}$ [or bx$^{b-1}$] and the other variable a [or b] is unbounded.
    \newline
    Thus, $\frac{f(x)}{g(x)}$ is bounded by $\frac{1}{a}$ for all x as described from the graph. So, the algorithm is as follows:
    \begin{algorithm}[H]
    Step-1: Obtain a sample \textbf{y} from \textbf{Y} distribution i.e. from g(x), where Y $\sim$ U$^\frac{1}{a}$.\;
    \\Step-2: Generate a sample \textbf{u} from the distribution U[0, Y$^{a-1}$].\; 
    \\Step-3:  Check whether u $<$ f(y) or not.
    \\\hfill Step-3a: If this condition holds, Accept \textbf{y} as a sample drawn from f function.\;
    \\\hfill Step-3b: Else, Reject \textbf{y}, return to the step-1 and repeat the procedure.\;
    \caption{Rejection Sampling Procedure-I}
    \end{algorithm}
\end{itemize}

\section{Question 3}
Consider a stream of discrete independent values being generated by a sensor. Assume the sensor has an ’alphabet’ of k different values and has an entropy of h $>$ 0. Let n denote the length (number of values) of the stream. We build a compression protocol for this stream as follows. Note that (at least the way we will present it here) is not a ’practical’ algorithm, though it can be turned into one with some additional work. The scheme here only illustrates the ’in-principle’ feasibility of a near-optimal compression scheme.
\begin{enumerate}[label=\alph*]
    \item Show the following:
    \begin{equation*}
    \binom{n}{np_{1},...,np_{k}} \leq 2^{nh}
    \end{equation*}
    where p$_{i}$ is the probability of the i$^{th}$ symbol.
    \item Prove a Chernoff bound on the probability that there is at least one 1 $\leq$ i $\leq$ k for which the number of occurrences of the i$^{th}$ value is too far away from the expected number np$_{i}$ ,i.e., prove an upper bound on 
    \begin{equation*}
    p( \lvert n_{i} - np_{i} \rvert > \epsilon.np_{i})
    \end{equation*}
    for some $\epsilon$ $>$ 0.
    \item Assume we have a ’dictionary’ D of strings that can potentially come from the stream with the number of occurrences of the i$_{th}$ symbol not more than $\epsilon$.np$_{i}$ away from the actual expected number np$_{i}$ , for every 1 $\leq$ i $\leq$ k. Show that the size of this dictionary is at most:
    \begin{equation*}
        O(c_{k}.2_{nh})
    \end{equation*}
    for a sufficiently small choice of $\epsilon$ and c $\approx$ (2$\epsilon$np$_{i}$ )
    \item Our compression protocol is as follows: given any string S from the sensor, we ’compress’ it to S' where
    \begin{equation*}
    S'= 
        \begin{cases}
            0S, & \text{if } S \notin D\\
            1(\#S),  & \text{if } S \in D
        \end{cases}
    \end{equation*}
    where (\#S) is the dictionary id (assume each member of the dictionary has a unique id) for the string. Note that when S $\notin$ D we are simply keeping S as is. The first bit is a flag that tells whether the ’compressed’ string is of the first or the second kind. Show that the expected length of S' is upper-bounded by (1 + $\delta$)nh for some small $\delta$ and a sufficiently large n.
    \item Discuss the tradeoffs in this compression scheme Argue that this is near-optimal.
\end{enumerate}

\subsection{Solution: }
\begin{enumerate}[label=\alph*]
    \item In the solution, we have been given a stream of discrete independent values generated by a sensor. The sensor has an alphabet $\Sigma$ = ( v$_{1}$, v$_{2}$,..., v$_{k}$ ) and entropy h $>$ 0. The length of stream is n. To Prove:
    \begin{equation*}
        \binom{n}{np_{1},...,np_{k}} \leq 2^{nh}
    \end{equation*}
    Evaluating RHS of the above equation:
    We know that Entropy is defined as
    \begin{equation*}
        h = -\sum_{x}p(x)\log{p(x)}
    \end{equation*}
    Substituting p$_{i}$ in the equation:
    \begin{equation*}
        h = -\sum_{i=1}^{k}p_{i}\log{p_{i}}
    \end{equation*}
    Expanding, 
    \begin{equation*}
        h = -(p_{1}\log{p_{1}} + p_{2}\log{p_{2}}  + ... + p_{k}\log{p_{k}})
    \end{equation*}
    Multiplying n into h in the former equation:
    \begin{equation*}
        nh = -(np_{1}\log{p_{1}} + np_{2}\log{p_{2}}  + ... + np_{k}\log{p_{k}})
    \end{equation*}
    Raising nh to power of 2:
    \begin{equation*}
        2^{nh} = 2^{-(np_{1}\log{p_{1}} + np_{2}\log{p_{2}}  + ... + np_{k}\log{p_{k}})}
    \end{equation*}
    \begin{equation*}
        2^{nh} = 2^{-np_{1}\log{p_{1}}} * 2^{-np_{2}\log{p_{2}}}  * ... * 2^{-np_{k}\log{p_{k}}}
    \end{equation*}
    \begin{equation*}
        2^{nh} = 2^{\log{p_{1}}^{-np_{1}}} * 2^{\log{p_{2}}^{-np_{2}}}  * ... * 2^{\log{p_{k}}^{-np_{k}}}
    \end{equation*}
    Simplifying using the property of logarithm:
    \begin{equation*}
        2^{nh} = \frac{1}{p_{1}^{np_{1}}} * \frac{1}{p_{2}^{np_{2}}}  * ... * \frac{1}{p_{k}^{np_{k}}}
    \end{equation*}
    \begin{equation}
        2^{nh} = p_{1}^{-np_{1}} * p_{2}^{-np_{2}}  * ... * p_{k}^{-np_{k}}
    \end{equation}
    Evaluating LHS, we get 
    \begin{equation*}
    \binom{n}{np_{1},...,np_{k}} = \frac{n\,!}{(np_{1})\,!(np_{2})\,! ... (np_{k})\,!}
    \end{equation*}
    
    \begin{equation*}
    \binom{n}{np_{1},...,np_{k}} = \binom{np_{1}}{np_{1}}\binom{np_{1} + np_{2}}{np_{2}}...\binom{np_{1} + np_{2} +... + np_{k}}{np_{k}}
    \end{equation*}
    We know that 
    \begin{equation*}
        n\,! \leq O(n^{n})
    \end{equation*}
    So, the above equation can be written as
    \begin{equation*}
    \binom{n}{np_{1},...,np_{k}} \leqslant \frac{n^{n}}{(np_{1})^{np_{1}},...,(np_{k})^{np_{k}}} 
    \end{equation*}
    \begin{equation}
    \binom{n}{np_{1},...,np_{k}} \leqslant \frac{n^{n}}{(n)^{np_{1}}(p_{1})^{np_{1}},...,(n)^{np_{k}}(p_{k})^{np_{k}}} 
    \end{equation}
    Thus, comparing the final results from final equations 4$^{th}$ of RHS and 5$^{th}$ of LHS, we can say that 
    \begin{equation*}
    \binom{n}{np_{1},...,np_{k}} \leq 2^{nh}
    \end{equation*}
    Hence, proved
    \item 
    To prove Chernoff bound on the probability that there is atleast one 1 $\leq$ i $\leq$ k for which 
    \begin{equation*}
    p( \lvert n_{i} - np_{i} \rvert > \epsilon.np_{i})
    \end{equation*}
    where n$_{i}$ is the number of occurrences of i$^{th}$ value and $\epsilon$ $>$ 0. We can say that
    \begin{equation*}
        p( \lvert n_{i} - np_{i} \rvert > \epsilon.np_{i}) = p(n_{i} > (1+\epsilon)np_{i} \cup n_{i} < (1-\epsilon)np_{i})
    \end{equation*}
    \begin{equation*}
        p( \lvert n_{i} - np_{i} \rvert > \epsilon.np_{i}) = 2(\text{Worst of the two bounds})
    \end{equation*}
    \begin{itemize}
        \item For p(n$_{i} > (1+\epsilon) np_{i}$), let us assume a = (1+$\epsilon$)np$_{i}$ and $\theta$ = np$_{i}$,
        \\From the property of Chernoff bounds, we can say that,
        \begin{equation*}
            p(n_{i} > (1+\epsilon) np_{i}) \leqslant e^{a-\theta-a\ln{\frac{a}{\theta}}}
        \end{equation*}
        \begin{equation*}
            \leqslant e^{(1+\epsilon)np_{i} - np_{i} - (1+\epsilon)np_{i}\ln{\frac{(1+\epsilon)np_{i}}{np_{i}}}}
        \end{equation*}
        \begin{equation*}
            \leqslant e^{np_{i}(\epsilon - (1+\epsilon)\ln{{(1+\epsilon)}})}
        \end{equation*}
        which can be concluded as:
        \begin{equation*}
            \leqslant e^{\frac{-np_{i}\epsilon^{2}}{3}}
        \end{equation*}
        This is a loose bound as the former term will decrease as $\epsilon$ increases.
        \item For p(n$_{i} < (1+\epsilon) np_{i}$), assuming t$<$0
        \begin{equation*}
            p(n_{i} < (1+\epsilon) np_{i}) = p(tX > ta)  
        \end{equation*}
        where X = n$_{i}$ and a = (1+$\epsilon$)np$_{i}$
        \begin{equation*}
            = p(e^{tX} > e^{ta})
        \end{equation*}
        \begin{equation*}
            \leqslant \frac{\prod_{i}E[e^{tX_{i}}]}{e^{ta}}
        \end{equation*}
        \begin{equation*}
            \leqslant \frac{\prod_{i}[(1-p_{i}) + p_{i}e^{t}]}{e^{ta}}
        \end{equation*}
        \begin{equation*}
            \leqslant e^{(\sum_{i}p_{i})(e^{t} - 1) - ta}
        \end{equation*}
        Let $\theta$ = $\sum_{i}p_{i}$ and differentiating wrt t,
        we get t = $\ln{\epsilon}$.
        Hence, the bound is
        \begin{equation*}
            \leqslant e^{\theta(-\epsilon - (1-\epsilon)\ln{1-\epsilon})}
        \end{equation*}
        \begin{equation*}
            \leqslant e^{-\frac{\theta \epsilon^{2}}{2}}
        \end{equation*}
    \end{itemize}
    Thus from the two bounds, we can conclude that 
    \begin{equation*}
        p( \lvert n_{i} - np_{i} \rvert > \epsilon.np_{i}) \leqslant 2e^{-\frac{\theta\epsilon^{2}}{3}}
    \end{equation*}
    where $\theta$ = np$_{i}$
\end{enumerate}

\end{document}
