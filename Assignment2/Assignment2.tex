\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{flafter}
\usepackage{subfig}
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage[algo2e]{algorithm2e}
%\graphicspath{ {/Users/tarinichandra/Desktop/Pothole-Detection} }

\begin{document}

\begin{titlepage}
	\centering
	%\includegraphics[width=0.15\textwidth]{IIIT-B_logo.jpg}\par\vspace{1cm}
	{\scshape\LARGE International Institute of Information Technology Bangalore \par}
	\vspace{1cm}
	{\scshape\Large Endsem Assignment\par}
	{\Large Foundation of Big Data Algorithms\par}
	\vspace{1.5cm}
	{\Large\itshape Akanksha Dwivedi - MT2016006\par}
	{\Large\itshape Kuldeep Singh - MT2016073\par}
	{\Large\itshape Tarini Chandrashekhar - MT2016144\par}
	\vfill
	
	Faculty : \par
	Prof. G Srinivasa Raghavan

	\vfill

% Bottom of the page
	{\large \today\par}
\end{titlepage}


\tableofcontents
%\listoffigures
%\listoftables
\newpage
\section{Question 3}
\subsection{Question 3c.}

"D" = Dictionary of strings
As we already proved in question 3a),

\begin{equation*}
    Upper bound on \textit{number of ways to pick k-samples out of length n stream} = 2^{nh}
\end{equation*}

As we later proved by Chernoff's bound in 3b), upper bound on \textit{difference between expected number of i-symbols and the actual number of i-symbols in a length k sample } is given by:
\begin{equation*}
    p(|n_i-np_i| > \epsilon p_i) \leq \frac{\theta\epsilon^2}{2}
\end{equation*}
where $\theta = \Sigma = p_i$

$\Sigma p_i$ can be written as $np_i$ so, that makes the upper bound $\frac{np_i\epsilon^2}{2}$. We want the $\epsilon$ to be as small as possible so, we differentiate it w.r.t $\epsilon$, which gives us:

\begin{equation*}
    c = \epsilon np_i
\end{equation*}
This bound is for one sample, so for k samples, the bound becomes c x c x ...k times i.e $c^k$.

Total length of D is the product of both upper bounds, i.e
\begin{equation*}

length(D) = c^k.2^{nh} = O(c^k.2^{nh})    
\end{equation*}


\section*{Question 4}
Let $u = {x_1,x_2}$ , i.e. u is 2-D Gaussian vector, so it shows a bivariate normal distribution $\Nu(x,\mu,\Sigma)$ where, 

\begin{equation*}
\mu = \begin{bmatrix}
\mu_1\\ 
\mu_2
\end{bmatrix}
\end{equation*}

\begin{equation*}
\Sigma = \begin{bmatrix}
\Sigma_{11} & \Sigma_{12} \\ 
\Sigma_{21} & \Sigma_{22} 
\end{bmatrix}
\end{equation*}

Note that since $\Sigma$ is a symmetric matrix, $\Sigma = \Sigma^T$.
The joint density function of u is :

\begin{equation*}
u(x) = u(x_1,x_2) = \frac{1}{(2\pi )^{n/2}|\Sigma|^{1/2}}exp[-\frac{1}{2}(x-\mu))^T\Sigma^{-1}(x-\mu)] =  \frac{1}{(2\pi )^{n/2}|\Sigma|^{1/2}}exp[-\frac{1}{2}Q(x_1,x_2)]
\end{equation*}

where Q is defined as:

\begin{equation*}

Q(x_1,x_2) = (x-\mu)^T\Sigma^{-1}(x-\mu) 
		 = [(x_1-\mu_1)^T,(x_2-\mu_2)^T]\begin{bmatrix}\Sigma^{11} & \Sigma^{12} \\ 
\Sigma^{21} & \Sigma^{22} 
\end{bmatrix} \begin{bmatrix}x_1-\mu_1 \\ x_2-\mu_2 \end{bmatrix}

\end{equation*}

We assume that,
\begin{equation*}
\Sigma^{-1} = \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\ 
\Sigma_{21} & \Sigma_{22} 
\end{bmatrix}^{-1}  = \begin{bmatrix}\Sigma^{11} & \Sigma^{12} \\ 
\Sigma^{21} & \Sigma^{22} 
\end{bmatrix}_1
\end{equation*}

So, in order to find the conditional distribution of $x_1$ with respect to $x_2$ and vice-versa, we use the conditional probability theorem, i.e

\begin{equation*}
f(x_1|x_2) = \frac{f(x_1,x_2)}{f(x_2} , and
\end{equation*}

\begin{equation*}
f(x_2|x_1) = \frac{f(x_1,x_2)}{f(x_1)}
\end{equation*} 

where we already defined the joint density function $f(x_1,x_2)$ above and now, we define the individual distributions for $x_1$ and $x_2$, assuming that both components of the 2-D Gaussian vector, u, are already normal with aforementioned defined fixed means and covariance matrices with respect to one another. 

\begin{equation*}
f(x_1) = \frac{1}{(2\pi )^{c/2}|\Sigma|^{1/2}}exp[-\frac{1}{2}(x_1-\mu_1))^T\Sigma_{11}^{-1}(x_1-\mu_1)] 
\end{equation*}
\begin{equation*}
f(x_2) = \frac{1}{(2\pi )^{d/2}|\Sigma|^{1/2}}exp[-\frac{1}{2}(x_2-\mu_2))^T\Sigma_{22}^{-1}(x_2-\mu_2)]
\end{equation*}
assuming c and d are dimensions of $x_1$ and $x_2$ components of u.

So,
\begin{equation*}
f(x_1|x_2) = \frac{ \frac{1}{(2\pi )^{n/2}|\Sigma|^{1/2}}exp[-\frac{1}{2}Q(x_1,x_2)]}{\frac{1}{(2\pi )^{d/2}|\Sigma|^{1/2}}exp[-\frac{1}{2}(x_2-\mu_2))^T\Sigma_{22}^{-1}(x_2-\mu_2)]}
\end{equation*}

\begin{equation*}
f(x_2|x_1) = \frac{\frac{1}{(2\pi )^{n/2}|\Sigma|^{1/2}}exp[-\frac{1}{2}Q(x_1,x_2)]}{\frac{1}{(2\pi )^{c/2}|\Sigma|^{1/2}}exp[-\frac{1}{2}(x_1-\mu_1))^T\Sigma_{11}^{-1}(x_1-\mu_1)] 
}
\end{equation*}

Instead of drawing from a proposed distribution from each dimension, then accepting/rejecting it, in Gibbs sampler, we get a value for that dimension according the variable's conditional distribution, where we calculate a value for a particular dimension, keeping all other values fixed.Let t be the number of states. Following is the algorithm:

\begin{algorithm}[H]
\SetAlgoLined
 1. Set t = 0.\\
 2. Generate an initial state $x^{(0)}$.\\
 3. Repeat k-times. \\
\tab set t = t + 1\\
\tab for each dimension i= 1..D  (\textit{Here, D=2})\\
\tab draw $x_i$ from $p(x_i|x_1,x_2,...,x_{i-1},x_{i+1},...,x_D)$

\end{algorithm}

In order to sample from this distribution using a Gibbs sampler, we need to have in hand the conditional distributions for variables/dimensions $x_1$ and $x_2$:

So, we calculate $p(x_1|x_2^{(t-1)})$ i.e the conditional for component $x_1$ for state t, keeping the component $x_2$ for the previous state t-1 constant,since the value of $x_2$ for state t isn't calculated yet. After this, we calculate $p(x_2|x_1^{(t)})$, keeping the recently calculated $x_1^{(t})$ fixed. 

This is how we use Gibbs sampling to sample the 2-D Gaussian vector u.
\section{Question 5}
Devise a rejection sampling procedure to sample from a beta distribution (a common distribution in ML during Bayesian analysis). The density of the Beta distribution, defined on [0, 1] is given as:
\begin{equation*}
    p(x) = \frac{1}{\beta(a,b)}x^{a-1}(1-x)^{b-1}
\end{equation*}
where $\beta$(a, b) is the beta function defined as $\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$. $\Gamma$(x) is the Gamma function - the generalization of the factorial for real numbers. The gamma function has the nice property that $\Gamma$(x + 1) = x$\Gamma$(x). Try to be creative in the choice of the reference function for the rejection sampling.

\subsection{Solution: }
To develop a rejection sampling procedure from a beta distribution whose density function is
\begin{equation}
    p(x) = \frac{1}{\beta(a,b)}x^{a-1}(1-x)^{b-1}
\end{equation}
where 
\begin{equation*}
    \beta(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}
\end{equation*}
The equation 1 can be written as:
\begin{equation}
    p(x) = \frac{f(x)}{constant}
\end{equation}
and
\begin{equation}
    f(x) = x^{a-1}(1-x)^{b-1}
\end{equation}
Now, the rejection sampling procedure for beta distribution can be described in two steps:
\begin{itemize}
    \item If \textbf{a $\geq$ 1}, \textbf{b $\geq$ 1}:
    \newline
    In this case, the beta density function is bounded i.e. we can bound the beta distribution with a uniform envelope with height as maximum value of f(x). To find the maximum value of f(x), differentiate f(x) wrt x and equating to 0.
    \begin{equation*}
        \frac{d}{dx}f(x) = x^{a-1}(1-x)^{b-1} = 0
    \end{equation*}
    \begin{equation*}
        (a-1)x^{a-2}(1-x)^{b-1} = x^{a-1}(b-1)(1-x)^{b-2}
    \end{equation*}
    \begin{equation*}
        x = \frac{a-1}{a+b-2}
    \end{equation*}
    So, the maximum value of f(x) is Max, as follows:
    \begin{equation*}
        Max = \frac{(a-1)^{a-1}(b-1)^{b-1}}{(a+b-2)^{a+b-2}}
    \end{equation*}
    \newline
    and the bounding uniform envelope around beta distribution is a bounding rectangle with height as the above Max value. We generate points uniformly inside this rectangle and those above the beta distribution region are rejected. Thus, rejection sampling procedure is as follows:
    \begin{algorithm}[H]
    Step-1: Obtain a sample \textbf{y} from \textbf{Y} distribution where Y $\sim$ U[0,1] and U denotes the uniform distribution with unit interval.\;
    \\Step-2: Generate a sample \textbf{u} from the distribution U[0, Max].\; 
    \\Step-3:  Check whether u $<$ f(y) or not.
    \\\hfill Step-3a: If this condition holds, Accept \textbf{y} as a sample drawn from f function.\;
    \\\hfill Step-3b: Else, Reject \textbf{y}, return to the step-1 and repeat the procedure.\;
    \caption{Rejection Sampling Procedure-I}
    \end{algorithm}
    
    \item
    If \textbf{a} $\in$ (0,1) and \textbf{b $\geq$ 1} or \textbf{b} $\in$ (0,1) and \textbf{a $\geq$ 1}, then we cannot get a flat bounding envelope, we will take Y = U$^{\frac{1}{a}}$ [or U$^{\frac{1}{b}}$] where U $\sim$ U[0,1] and g(x) = ax$^{a-1}$ [or bx$^{b-1}$] and the other variable a [or b] is unbounded.
    \newline
    Thus, $\frac{f(x)}{g(x)}$ is bounded by $\frac{1}{a}$ for all x as described from the graph. So, the algorithm is as follows:
    \begin{algorithm}[H]
    Step-1: Obtain a sample \textbf{y} from \textbf{Y} distribution i.e. from g(x), where Y $\sim$ U$^\frac{1}{a}$.\;
    \\Step-2: Generate a sample \textbf{u} from the distribution U[0, Y$^{a-1}$].\; 
    \\Step-3:  Check whether u $<$ f(y) or not.
    \\\hfill Step-3a: If this condition holds, Accept \textbf{y} as a sample drawn from f function.\;
    \\\hfill Step-3b: Else, Reject \textbf{y}, return to the step-1 and repeat the procedure.\;
    \caption{Rejection Sampling Procedure-I}
    \end{algorithm}
\end{itemize}

\end{document}
